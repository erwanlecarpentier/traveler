/**
 * @brief Simulation parameters
 */
id = 0
simulation_limit_time = 1000
nb_simulations = 100
backup_path = "data/backup0.csv"

/**
 * @brief Environment parameters
 *
 * Generate map:
 * true: generate a random map
 * false: use the graph duration matrix to generate the map
 */
reward_scaling_max = 1000.0 ///< Duration after which reaching the goal leads to reward 0
goal_reward = 0.0 ///< Bonus reward when reaching the goal
dead_end_reward = -10000.0 ///< Penalty reward when reaching dead-end
noop_reward = 0.0 ///< deprecated

generate_map = true

/**
 * Parameters for auto-generated map
 *
 * Sampler selector:
 * 0: 1st order random uniform (this is default)
 * 1: 2nd order random uniform
 * 2: 2nd order epsilon-random uniform
 * 3: cos heuristic
 *
 * Graph type selector for randomly generated graphs:
 * 0: build a connected directed graph (this is default)
 * 1: build a connected symmetric directed graph
 * 2: build a sequential graph
 */
sampler_selector = 3
graph_type_selector = 2
nb_time_steps = 100 // Number of time-steps in each edge history
time_steps_width = 10 // Time-steps width

// Parameters for graph type 0 1
nb_nodes = 50
min_nb_edges_per_node = 8

// Parameters for graph type 2
nb_links = 5
nb_nodes_per_link = 5

duration_min = 0.0 // Maximum duration within an edge
duration_max = 100.0 // Minimum duration within an edge
lip = 1. // Lipschitz constant ie maximum duration variation between subsequent time steps
save_duration_matrix = true
output_duration_matrix = "config/map.csv"

/** Parameters for loaded duration matrix */
input_duration_matrix = "config/map.csv"
initial_location = "n0" // default
terminal_location = "n1" // default
csv_sep = ";" // default

/**
 * @brief Policy parameters
 *
 * Policy selector:
 * 0: random
 * 1: MCTS
 * 2: UCT
 * 3: TMP_MCTS
 * 4: TMP_UCT
 */
policy_selector = 2
is_model_dynamic = true
discount_factor = 1.0
uct_parameter = 0.707
tree_search_budget = 10000
default_policy_horizon = 100

regression_regularization = 0.
polynomial_regression_degree = 1

